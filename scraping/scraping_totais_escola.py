import os
import time
import logging
import locale
import json
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup

load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

try:
    locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')
except Exception:
    try:
        locale.setlocale(locale.LC_ALL, '')
    except Exception:
        pass

FORM_URL = os.environ.get("FORM_URL")
REPORT_URL = os.environ.get("REPORT_URL")
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
TABELA_PRINCIPAL = "DadosEscolas"

BATCH_SIZE = 50
ANO_ATUAL = str(datetime.now().year)

if not all([FORM_URL, REPORT_URL, SUPABASE_URL, SUPABASE_KEY]):
    raise ValueError("Faltam vari√°veis de ambiente obrigat√≥rias.")

# Inicializa Supabase
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def get_session():
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    # Tenta 3 vezes se receber erros de servidor ou falha de conex√£o
    retry_strategy = Retry(
        total=3,
        backoff_factor=1, # Espera 1s, 2s, 4s...
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET", "POST"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session

session = get_session()

# Par√¢metros Base
PARAMS_RELATORIO = {
    'tipo': '0', 'anoInicial': ANO_ATUAL, 'anoFinal': ANO_ATUAL,
    'situacao_pagamento': '0', 'repasse_cre': '0'
}

# --- Fun√ß√µes de Extra√ß√£o (Scraping) ---

def fetch_content(url: str, params: dict = None):
    """Busca conte√∫do com tratamento de erro e retry autom√°tico"""
    try:
        response = session.get(url, params=params, timeout=30)
        response.raise_for_status()
        response.encoding = 'iso-8859-1'
        return response.text
    except Exception as e:
        logging.error(f"Erro de rede ao acessar {url}: {e}")
        return None

def get_options(select_id: str, html: str):
    """Extrai op√ß√µes de um select"""
    if not html: return []
    soup = BeautifulSoup(html, 'html.parser')
    select = soup.find('select', {'id': select_id})
    if not select: return []
    
    return [
        {'value': opt.get('value'), 'text': opt.text.strip()}
        for opt in select.find_all('option')
        if opt.get('value') and opt.get('value') not in ["0", ""]
    ]

def parse_valor_total(html: str) -> float:
    """Extrai o valor monet√°rio do HTML"""
    if not html: return 0.0
    soup = BeautifulSoup(html, 'html.parser')
    
    # Procura por "Total Geral" (insens√≠vel a mai√∫sculas/min√∫sculas)
    target = soup.find(lambda tag: tag.name == "b" and "total geral" in tag.get_text(strip=True).lower())
    
    if target:
        # Tenta pegar o pr√≥ximo elemento, que geralmente cont√©m o valor
        proximo = target.find_next('b')
        if proximo:
            valor_texto = proximo.get_text(strip=True).replace("R$", "").strip()
            try:
                # Converte "1.234,56" para float 1234.56
                return float(valor_texto.replace('.', '').replace(',', '.'))
            except ValueError:
                return 0.0
    return 0.0

def processar_escola(regional, municipio, escola):
    """Processa uma √∫nica escola e retorna o dicion√°rio de dados ou erro"""
    try:
        # L√≥gica para separar C√≥digo MEC do Nome
        parts = escola['text'].split(" - ", 1)
        codigo_mec = parts[0].strip() if len(parts) > 1 else None
        nome_escola = parts[1].strip() if len(parts) > 1 else escola['text']

        # Busca dados do relat√≥rio
        params = PARAMS_RELATORIO.copy()
        params.update({
            'subsecretaria': regional['value'],
            'Municipio': municipio['value'],
            'Escola': escola['value']
        })
        
        html = fetch_content(REPORT_URL, params)
        valor = parse_valor_total(html)

        dados = {
            "id_mec": codigo_mec,
            "id_interno": escola['value'],
            "nome_escola": nome_escola,
            "municipio": municipio['text'],
            "regional": regional['text'],
            "total_valor": valor,
            "ano_referencia": int(ANO_ATUAL),
            "data_extracao": datetime.now().isoformat()
        }
        return dados, None

    except Exception as e:
        erro_info = {
            "escola": escola['text'],
            "id_interno": escola['value'],
            "erro": str(e),
            "data": datetime.now().isoformat()
        }
        return None, erro_info

# --- Fun√ß√µes de Banco de Dados (Batch Operations) ---

def flush_dados(buffer_dados):
    """Salva um lote de dados no Supabase (Hist√≥rico e Atualiza√ß√£o da Tabela Principal)"""
    if not buffer_dados: return

    logging.info(f"üíæ Salvando lote de {len(buffer_dados)} registros...")
    
    try:
        supabase.table("historico_scrapes").insert(buffer_dados).execute()

        ids_coletados = [item['id_mec'] for item in buffer_dados if item['id_mec']]
        
        if not ids_coletados:
            return

        response_existentes = supabase.table(TABELA_PRINCIPAL)\
            .select("Id")\
            .in_("Id", ids_coletados)\
            .execute()
        
        ids_validos = {row['Id'] for row in response_existentes.data}
        
        dados_update = []
        for item in buffer_dados:
            if item['id_mec'] and item['id_mec'] in ids_validos:
                dados_update.append({
                    "Id": item['id_mec'],
                    "investimento_ano_atual": item['total_valor'],
                })
        
        if dados_update:
            supabase.table(TABELA_PRINCIPAL).upsert(
                dados_update, on_conflict="Id"
            ).execute()

            ignorado_count = len(ids_coletados) - len(dados_update)
            logging.info(f"‚úÖ Atualizados: {len(dados_update)} | Ignorados (n√£o existem): {ignorado_count}")
        else:
            logging.info("Nenhum ID deste lote existia na tabela principal. Nada atualizado.")
        
    except Exception as e:
        logging.error(f"‚ùå Erro cr√≠tico ao salvar lote no Supabase: {e}")

# --- Loop Principal ---

def main():
    logging.info(f"üöÄ Iniciando scraping otimizado | Ano: {ANO_ATUAL}")
    
    buffer_dados = []
    lista_erros = []
    total_processado = 0
    
    html_ini = fetch_content(FORM_URL)
    regionais = get_options('cmbSubsecretaria', html_ini)

    for regional in regionais:
        logging.info(f"üìç Regional: {regional['text']}")
        
        html_mun = fetch_content(FORM_URL, {'cmbSubsecretaria': regional['value']})
        municipios = get_options('cmbMunicipio', html_mun)

        for municipio in municipios:
            logging.info(f"  üèôÔ∏è Munic√≠pio: {municipio['text']}")
            
            html_esc = fetch_content(FORM_URL, {
                'cmbSubsecretaria': regional['value'],
                'cmbMunicipio': municipio['value']
            })
            escolas = get_options('cmbUnidadeEnsino', html_esc)

            for escola in escolas:
                # Processa escola individualmente
                dados, erro = processar_escola(regional, municipio, escola)
                
                if dados:
                    buffer_dados.append(dados)
                    logging.info(f"    -> {dados['nome_escola'][:30]}... : R$ {dados['total_valor']}")
                    total_processado += 1
                elif erro:
                    lista_erros.append(erro)
                    logging.error(f"    ‚ùå Erro em {escola['text']}")

                # FLUSH: Se o buffer encher (50 itens), salva no banco e limpa mem√≥ria
                if len(buffer_dados) >= BATCH_SIZE:
                    flush_dados(buffer_dados)
                    buffer_dados = [] # Limpa buffer
                
                # Delay reduzido pois agora temos retries e batch saving
                time.sleep(0.1)

    # Flush final (salva o que sobrou no buffer)
    if buffer_dados:
        flush_dados(buffer_dados)

    # Relat√≥rio de Erros
    if lista_erros:
        logging.warning(f"‚ö†Ô∏è Processo finalizado com {len(lista_erros)} erros.")
        with open('erros_extracao.json', 'w', encoding='utf-8') as f:
            json.dump(lista_erros, f, ensure_ascii=False, indent=2)
    else:
        logging.info("‚ú® Processo finalizado com SUCESSO ABSOLUTO!")
    
    logging.info(f"üìä Total de registros processados: {total_processado}")

if __name__ == "__main__":
    main()